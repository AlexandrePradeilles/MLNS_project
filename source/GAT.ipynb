{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dgl.sparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dgl==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from functions_HGPLS import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/SMILES_tokenized_PubChem_shard00_160k\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/SMILES_tokenized_PubChem_shard00_160k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "atoms = [\"\", \"H\", \"He\", \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\", \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\", \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Xe\", \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\", \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Fr\", \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\", \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\"]\n",
    "print(len(atoms))\n",
    "ids = tokenizer(atoms).input_ids\n",
    "\n",
    "toks = list()\n",
    "for i in ids:\n",
    "    if len(i) >= 3:\n",
    "        toks.append(i[1])\n",
    "    else:\n",
    "        toks.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = dgl.load_graphs(\"../data/HIV_dgl_graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels['glabel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dataset[0].ndata[\"atomic\"]\n",
    "tok = torch.zeros((len(toks), 1))\n",
    "for i in range(len(toks)):\n",
    "    tok[i] = toks[i]\n",
    "\n",
    "tok\n",
    "transfo_atoms = model(tok.to(torch.long)).logits.reshape(100, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, random_state=42)\n",
    "res = pca.fit_transform(transfo_atoms.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = list()\n",
    "for i in range(len(dataset)):\n",
    "    #g, l = dataset[i]\n",
    "    g = dataset[i]\n",
    "    l = labels[i]\n",
    "    #g.ndata[\"feature\"] = torch.ones_like(dataset.feature[g.ndata[\"_ID\"]])\n",
    "    ids=list(g.ndata[\"atomic\"].numpy().flatten().astype(int))\n",
    "    g.ndata[\"feature\"] = torch.tensor(res[ids]) #g.ndata[\"atomic\"]\n",
    "    g = dgl.add_self_loop(g)\n",
    "    g = dgl.add_reverse_edges(g)\n",
    "    new_dataset.append((g, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(new_dataset, test_size=0.25, random_state=42)\n",
    "\n",
    "train_dataloader = GraphDataLoader(train_dataset, batch_size=16, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(test_dataset, batch_size=16, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, n_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GATConv(in_feats, h_feats, num_heads=4)\n",
    "        self.layer2 = GATConv(4*h_feats, h_feats, num_heads=4)\n",
    "        self.layer3 = GATConv(4*h_feats, h_feats, num_heads=6)\n",
    "        self.fc = nn.Linear(h_feats, n_classes)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        x1 = self.layer1(g, in_feat)\n",
    "        x1 = self.elu(x1)\n",
    "        x1 = x1.view(in_feat.shape[0], -1)\n",
    "        x2 = self.layer2(g, x1)\n",
    "        x2 = self.elu(x2)\n",
    "        x2 = x2.view(in_feat.shape[0], -1)\n",
    "        x3 = self.layer3(g, x2)\n",
    "        x3 = torch.mean(x3, dim=1)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = x3\n",
    "            x4 = dgl.readout_nodes(g, 'h')\n",
    "        return F.log_softmax(self.fc(x4), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model architecture\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "model = GAT(in_feats=100, n_classes=3, h_feats=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../models/GATModel_prot.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.000002 #, weight_decay=0.0001\n",
    "    )\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:16<00:00,  6.80it/s]\n",
      "100%|██████████| 39/39 [00:02<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=30.1189, train_acc=0.3564, val_acc=0.3961, vall_loss=6.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 39/115 [00:07<00:14,  5.10it/s]"
     ]
    }
   ],
   "source": [
    "# Train model and keep the best validation loss model\n",
    "bad_cound = 0\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "epochs = 40\n",
    "patience = 10\n",
    "print_every = 1\n",
    "train_times = []\n",
    "for e in range(epochs):\n",
    "    s_time = time()\n",
    "    train_loss, train_acc = train(model, optimizer, loss, train_dataloader, device)\n",
    "    train_times.append(time() - s_time)\n",
    "    val_acc, val_loss = test(model, loss, test_dataloader, device)\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        bad_cound = 0\n",
    "        best_epoch = e + 1\n",
    "        torch.save(model.state_dict(), \"../models/GATModel_prot.pt\")\n",
    "    else:\n",
    "        bad_cound += 1\n",
    "    if bad_cound >= patience:\n",
    "        break\n",
    "\n",
    "    if (e + 1) % print_every == 0:\n",
    "        log_format = (\n",
    "            \"Epoch {}: train_loss={:.4f}, train_acc={:.4f}, val_acc={:.4f}, vall_loss={:.4f}\"\n",
    "        )\n",
    "        print(log_format.format(e + 1, train_loss, train_acc, val_acc, val_loss))\n",
    "print(\n",
    "    \"Best Epoch {}, final test loss {:.4f}\".format(\n",
    "        best_epoch, best_val_acc\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 31.11it/s]\n"
     ]
    }
   ],
   "source": [
    "pred=[]\n",
    "lab = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "        batch_graphs, batch_labels = batch\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        batch_labels = batch_labels.long().to(device)\n",
    "        out = model(batch_graphs, batch_graphs.ndata[\"feature\"].to(dtype=torch.float32))\n",
    "        pred += out.argmax(dim=1).tolist()\n",
    "        lab += batch_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[122,  46,  93],\n",
       "       [101,  54, 100],\n",
       "       [  9,  13,  73]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(lab, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
