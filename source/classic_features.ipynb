{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from functions_HGPLS import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/SMILES_tokenized_PubChem_shard00_160k\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/SMILES_tokenized_PubChem_shard00_160k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "atoms = [\"\", \"H\", \"He\", \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\", \"K\", \"Ca\", \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\", \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Xe\", \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\", \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\", \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\", \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Fr\", \"Ra\", \"Ac\", \"Th\", \"Pa\", \"U\", \"Np\", \"Pu\", \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\"]\n",
    "print(len(atoms))\n",
    "ids = tokenizer(atoms).input_ids\n",
    "\n",
    "toks = list()\n",
    "for i in ids:\n",
    "    if len(i) >= 3:\n",
    "        toks.append(i[1])\n",
    "    else:\n",
    "        toks.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels_g = dgl.load_graphs(\"../data/HIV_dgl_graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_g = labels_g['glabel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dataset[0].ndata[\"atomic\"]\n",
    "tok = torch.zeros((len(toks), 1))\n",
    "for i in range(len(toks)):\n",
    "    tok[i] = toks[i]\n",
    "\n",
    "tok\n",
    "transfo_atoms = model(tok.to(torch.long)).logits.reshape(100, -1)\n",
    "\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "res = pca.fit_transform(transfo_atoms.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(g):\n",
    "    degree = (g.out_degrees().float() + g.in_degrees().float()).numpy()\n",
    "    num_edges = g.number_of_edges()\n",
    "    num_nodes = g.number_of_nodes()\n",
    "    nx_g = g.to_networkx().to_undirected()\n",
    "    laplacian = nx.laplacian_matrix(nx_g).astype(np.float32).toarray()\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(laplacian)\n",
    "    return [np.mean(degree), np.max(degree)/len(degree), 100*num_edges / (num_nodes * (num_nodes - 1))] + list(eigenvals[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=22, num_edges=46,\n",
       "      ndata_schemes={'atomic': Scheme(shape=(1,), dtype=torch.float32)}\n",
       "      edata_schemes={'type': Scheme(shape=(1,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2443/2443 [00:00<00:00, 58171.54it/s]\n"
     ]
    }
   ],
   "source": [
    "new_dataset = list()\n",
    "labels = list()\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    g = dataset[i]\n",
    "    l = labels_g[i]\n",
    "    # features = list(np.mean(dataset.feature[g.ndata[\"_ID\"]].numpy(), axis=0))\n",
    "    # features = list(np.mean(g.ndata[\"attr\"].numpy(), axis=0))\n",
    "    # features = create_features(g)\n",
    "    # new_dataset.append(features)\n",
    "    ids=list(g.ndata[\"atomic\"].numpy().flatten().astype(int))\n",
    "    new_dataset.append(np.sum(res[ids], axis=0))\n",
    "    labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_labels, test_labels = train_test_split(new_dataset, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb =XGBClassifier(random_state=0)\n",
    "\n",
    "xgb.fit(train_dataset, train_labels)\n",
    "predxgb = xgb.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515548281505729"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, predxgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149, 104,   8],\n",
       "       [ 95, 133,  27],\n",
       "       [ 12,  29,  54]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(test_labels, predxgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
